{
  "permissions": {
    "allow": [
      "Bash(dir \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\" /B)",
      "Bash(pip show:*)",
      "Bash(git -C \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\" config user.email \"georgehadji@users.noreply.github.com\")",
      "Bash(git -C \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\" config:*)",
      "Bash(git -C \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\" add .gitignore __init__.py __main__.py api_clients.py cache.py cli.py engine.py models.py pyproject.toml README.md state.py validators.py)",
      "Bash(git -C \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\" status --short)",
      "Bash(git -C \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\" commit -m \"$\\(cat <<''EOF''\nInitial release: multi-LLM orchestrator v1.0.0\n\nAsync orchestration engine that decomposes projects into atomic tasks,\nroutes each to the optimal provider \\(OpenAI / Anthropic / Google\\), and\nruns cross-provider generate â†’ critique â†’ revise â†’ evaluate cycles.\n\nFeatures:\n- Unified async API client with retry, semaphore, and disk cache\n- JSON state persistence \\(SQLite WAL\\) with per-task checkpointing\n- Deterministic validators: python_syntax, pytest, ruff, json_schema, latex, length\n- Budget and time ceiling enforcement with phase partitioning\n- Resume from last checkpoint after crash or interruption\n- Topological task graph with Kahn''s algorithm for dependency ordering\n\nCo-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>\nEOF\n\\)\")",
      "Bash(gh repo create:*)",
      "Bash(pip install:*)",
      "Bash(dir:*)",
      "Bash(git mv:*)",
      "Bash(git add:*)",
      "Bash(git commit -m \"$\\(cat <<''EOF''\nfix: restructure as proper Python package under orchestrator/\n\nMove all source modules into orchestrator/ subdirectory so that\nsetuptools package discovery works correctly and pip install -e .\nsucceeds on any setuptools version >= 68.\n\nAlso fix build-backend from setuptools.backends._legacy \\(requires\nsetuptools >= 71\\) to the stable setuptools.build_meta.\n\nCo-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>\nEOF\n\\)\")",
      "Bash(git push)",
      "Bash(python -m orchestrator:*)",
      "Bash(python:*)",
      "WebFetch(domain:platform.moonshot.ai)",
      "Bash(git commit:*)",
      "Bash(git checkout:*)",
      "Bash(git push:*)",
      "Bash(gh pr create --title \"feat: add Kimi K2.5 provider + fix Windows CLI issues\" --body \"$\\(cat <<''EOF''\n## Summary\n\n- **New provider: Kimi K2.5** \\(Moonshot AI via `api.moonshot.cn`\\) â€” integrated using the OpenAI-compatible SDK with a custom `base_url`. Added to `ROUTING_TABLE` for `CODE_GEN`, `CODE_REVIEW`, `REASONING`, and `EVALUATE` task types, with `GPT-4o` as fallback.\n- **Fixed Windows terminal crash** in `cli.py` â€” replaced Unicode `âœ“`/`âœ—` emoji with ASCII `[OK  ]`/`[FAIL]`/`[DEG ]` markers to avoid `UnicodeEncodeError` on Windows \\(cp1252/cp1253 terminals\\).\n- **Fixed ruff false positives** in `validators.py` â€” added `--ignore=E402,F401` to suppress \"import not at top\" and \"unused import\" errors on multi-file LLM-generated code.\n- **Reliable `.env` loading** â€” added `load_dotenv\\(override=True\\)` at both `cli.py` module level and inside `_init_clients\\(\\)` to ensure API keys are always read before client initialization.\n- **Debug helpers** â€” added `check_outputs.py` and `check_state.py` scripts for inspecting project run results.\n\n## Test plan\n\n- [ ] Set `KIMI_API_KEY` in `.env` and verify `Kimi K2.5 client initialized` in logs\n- [ ] Run orchestrator on Windows and confirm no `UnicodeEncodeError` in output\n- [ ] Run a project with ruff validation and confirm E402/F401 no longer fail generated code\n- [ ] Run `python check_outputs.py` and `python check_state.py` after a project run\n\nðŸ¤– Generated with [Claude Code]\\(https://claude.com/claude-code\\)\nEOF\n\\)\")",
      "Bash(findstr:*)",
      "Bash(git -C \"E:/Documents/Vibe-Coding/Ai Orchestrator\" diff orchestrator/cli.py)",
      "Bash(gh pr:*)",
      "Skill(commit-commands:commit-push-pr)",
      "Skill(commit-commands:commit-push-pr:*)",
      "Bash(git rm:*)",
      "Bash(ls \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\orchestrator\"\" && cat \"E:DocumentsVibe-CodingAi OrchestratorREADME.md \")",
      "Bash(git stash:*)",
      "Bash(\"E:/Documents/Vibe-Coding/Ai Orchestrator/tests/test_telemetry_extended.py\" << 'PYEOF'\n\"\"\"\nExtended tests for TelemetryCollector.\nCovers latency buffer, p95, cost EMA, validator failures, error_rate,\ntrust dynamics, call_count, success_rate, and policy violations.\nAll tests are synchronous.\n\"\"\"\nfrom __future__ import annotations\n\nimport pytest\nfrom orchestrator.policy import ModelProfile\nfrom orchestrator.models import Model, TaskType, build_default_profiles\nfrom orchestrator.telemetry import TelemetryCollector\n\n\n# â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef _make_collector\\(model: Model = Model.GPT_4O\\):\n    profiles = build_default_profiles\\(\\)\n    collector = TelemetryCollector\\(profiles\\)\n    return collector, profiles, model\n\n\ndef _record\\(collector, model, latency_ms=500.0, cost_usd=0.01, success=True, quality=None\\):\n    collector.record_call\\(model, latency_ms, cost_usd, success, quality\\)\n\n\n# â”€â”€ Latency buffer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nclass TestLatencyBuffer:\n\n    def test_buffer_capped_at_50_after_60_samples\\(self\\):\n        collector, profiles, model = _make_collector\\(\\)\n        for i in range\\(60\\):\n            _record\\(collector, model, latency_ms=float\\(100 + i\\)\\)\n        assert len\\(profiles[model].latency_samples\\) == 50\n\n    def test_buffer_stays_sorted_ascending\\(self\\):\n        collector, profiles, model = _make_collector\\(\\)\n        import random\n        for _ in range\\(20\\):\n            _record\\(collector, model, latency_ms=random.uniform\\(50.0, 2000.0\\)\\)\n        buf = profiles[model].latency_samples\n        assert buf == sorted\\(buf\\)\n\n    def test_buffer_skips_zero_latency\\(self\\):\n        collector, profiles, model = _make_collector\\(\\)\n        _record\\(collector, model, latency_ms=0.0\\)\n        assert len\\(profiles[model].latency_samples\\) == 0\n\n    def test_buffer_skips_negative_latency\\(self\\):\n        collector, profiles, model = _make_collector\\(\\)\n        _record\\(collector, model, latency_ms=-10.0\\)\n        assert len\\(profiles[model].latency_samples\\) == 0\n\n    def test_multiple_models_have_independent_buffers\\(self\\):\n        collector, profiles, _ = _make_collector\\(\\)\n        for i in range\\(5\\):\n            _record\\(collector, Model.GPT_4O, latency_ms=100.0\\)\n        for i in range\\(10\\):\n            _record\\(collector, Model.CLAUDE_OPUS, latency_ms=200.0\\)\n        assert len\\(profiles[Model.GPT_4O].latency_samples\\) == 5\n        assert len\\(profiles[Model.CLAUDE_OPUS].latency_samples\\) == 10\n\n\n# â”€â”€ p95 tracking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nclass TestP95:\n\n    def test_real_p95_gte_900_for_10_samples_100_to_1000\\(self\\):\n        \"\"\"10 evenly-spaced samples 100â€“1000ms â†’ p95 should be at the high end.\"\"\"\n        collector, profiles, model = _make_collector\\(\\)\n        for v in [100.0, 200.0, 300.0, 400.0, 500.0,\n                  600.0, 700.0, 800.0, 900.0, 1000.0]:\n            _record\\(collector, model, latency_ms=v\\)\n        assert profiles[model].latency_p95_ms >= 900.0\n\n    def test_p95_is_not_double_avg\\(self\\):\n        \"\"\"Real p95 should not simply be 2Ã— the EMA avg \\(regression guard\\).\"\"\"\n        collector, profiles, model = _make_collector\\(\\)\n        for v in [100.0, 200.0, 300.0, 400.0, 500.0,\n                  600.0, 700.0, 800.0, 900.0, 1000.0]:\n            _record\\(collector, model, latency_ms=v\\)\n        p95 = profiles[model].latency_p95_ms\n        avg = profiles[model].avg_latency_ms\n        # If p95 == 2*avg exactly, something is wrong \\(that was the old formula\\)\n        assert abs\\(p95 - 2.0 * avg\\) > 1.0\n\n\n# â”€â”€ Cost EMA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nclass TestCostEMA:\n\n    def test_zero_cost_does_not_update_avg_cost_usd\\(self\\):\n        collector, profiles, model = _make_collector\\(\\)\n        original = profiles[model].avg_cost_usd  # default 0.0\n        _record\\(collector, model, cost_usd=0.0\\)\n        assert profiles[model].avg_cost_usd == original\n\n    def test_positive_cost_updates_avg_cost_usd\\(self\\):\n        collector, profiles, model = _make_collector\\(\\)\n        _record\\(collector, model, cost_usd=0.01\\)\n        # EMA with alpha=0.1: new = 0.1*0.01 + 0.9*0.0 = 0.001\n        assert profiles[model].avg_cost_usd > 0.0\n\n    def test_cost_ema_converges_after_20_identical_values\\(self\\):\n        \"\"\"After 20 records of cost=0.05, avg_cost_usd should be close to 0.05.\"\"\"\n        collector, profiles, model = _make_collector\\(\\)\n        # Seed with one value first so EMA has a non-zero start\n        _record\\(collector, model, cost_usd=0.05\\)\n        for _ in range\\(19\\):\n            _record\\(collector, model, cost_usd=0.05\\)\n        # After many identical samples the EMA converges toward the value\n        assert abs\\(profiles[model].avg_cost_usd - 0.05\\) < 0.01\n\n\n# â”€â”€ Validator failures â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nclass TestValidatorFailures:\n\n    def test_record_validator_failure_increments_count\\(self\\):\n        collector, profiles, model = _make_collector\\(\\)\n        assert profiles[model].validator_fail_count == 0\n        collector.record_validator_failure\\(model\\)\n        assert profiles[model].validator_fail_count == 1\n        collector.record_validator_failure\\(model\\)\n        assert profiles[model].validator_fail_count == 2\n\n    def test_record_validator_failure_unknown_model_no_crash\\(self\\):\n        \"\"\"Calling record_validator_failure for a non-profiled model should not raise.\"\"\"\n        profiles = {Model.GPT_4O: build_default_profiles\\(\\)[Model.GPT_4O]}\n        collector = TelemetryCollector\\(profiles\\)\n        # CLAUDE_OPUS is not in profiles dict â†’ should be a no-op\n        collector.record_validator_failure\\(Model.CLAUDE_OPUS\\)  # must not raise\n\n\n# â”€â”€ Error rate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nclass TestErrorRate:\n\n    def test_error_rate_zero_when_no_calls\\(self\\):\n        collector, _, model = _make_collector\\(\\)\n        assert collector.error_rate\\(model\\) == 0.0\n\n    def test_error_rate_one_when_all_calls_fail\\(self\\):\n        collector, _, model = _make_collector\\(\\)\n        for _ in range\\(5\\):\n            _record\\(collector, model, success=False\\)\n        assert collector.error_rate\\(model\\) == 1.0\n\n    def test_error_rate_half_when_half_fail\\(self\\):\n        collector, _, model = _make_collector\\(\\)\n        for _ in range\\(5\\):\n            _record\\(collector, model, success=True\\)\n        for _ in range\\(5\\):\n            _record\\(collector, model, success=False\\)\n        assert collector.error_rate\\(model\\) == pytest.approx\\(0.5\\)\n\n    def test_error_rate_unknown_model_returns_zero\\(self\\):\n        profiles = {Model.GPT_4O: build_default_profiles\\(\\)[Model.GPT_4O]}\n        collector = TelemetryCollector\\(profiles\\)\n        assert collector.error_rate\\(Model.CLAUDE_OPUS\\) == 0.0\n\n\n# â”€â”€ Quality EMA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nclass TestQualityEMA:\n\n    def test_quality_ema_unchanged_when_quality_none\\(self\\):\n        collector, profiles, model = _make_collector\\(\\)\n        initial = profiles[model].quality_score\n        _record\\(collector, model, quality=None\\)\n        assert profiles[model].quality_score == initial\n\n    def test_quality_ema_updates_when_score_provided\\(self\\):\n        collector, profiles, model = _make_collector\\(\\)\n        initial = profiles[model].quality_score\n        _record\\(collector, model, quality=1.0\\)\n        assert profiles[model].quality_score != initial\n\n\n# â”€â”€ Trust dynamics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nclass TestTrustDynamics:\n\n    def test_trust_degrades_on_failure\\(self\\):\n        collector, profiles, model = _make_collector\\(\\)\n        initial = profiles[model].trust_factor\n        _record\\(collector, model, success=False\\)\n        assert profiles[model].trust_factor < initial\n\n    def test_trust_recovers_on_success\\(self\\):\n        collector, profiles, model = _make_collector\\(\\)\n        # Degrade first\n        _record\\(collector, model, success=False\\)\n        degraded = profiles[model].trust_factor\n        _record\\(collector, model, success=True\\)\n        assert profiles[model].trust_factor > degraded\n\n    def test_trust_never_exceeds_cap\\(self\\):\n        collector, profiles, model = _make_collector\\(\\)\n        for _ in range\\(100\\):\n            _record\\(collector, model, success=True\\)\n        assert profiles[model].trust_factor <= 1.0\n\n\n# â”€â”€ Call count â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nclass TestCallCount:\n\n    def test_call_count_increments_each_call\\(self\\):\n        collector, profiles, model = _make_collector\\(\\)\n        assert profiles[model].call_count == 0\n        _record\\(collector, model\\)\n        assert profiles[model].call_count == 1\n        _record\\(collector, model\\)\n        assert profiles[model].call_count == 2\n\n\n# â”€â”€ Success rate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nclass TestSuccessRate:\n\n    def test_success_rate_via_rolling_window\\(self\\):\n        collector, profiles, model = _make_collector\\(\\)\n        for _ in range\\(5\\):\n            _record\\(collector, model, success=True\\)\n        for _ in range\\(5\\):\n            _record\\(collector, model, success=False\\)\n        # Rolling window of 10 â€” 5 success, 5 failure = 50%\n        assert profiles[model].success_rate == pytest.approx\\(0.5\\)\n\n    def test_success_rate_1_when_all_succeed\\(self\\):\n        collector, profiles, model = _make_collector\\(\\)\n        for _ in range\\(10\\):\n            _record\\(collector, model, success=True\\)\n        assert profiles[model].success_rate == 1.0\n\n\n# â”€â”€ Policy violation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nclass TestPolicyViolation:\n\n    def test_record_policy_violation_degrades_trust\\(self\\):\n        collector, profiles, model = _make_collector\\(\\)\n        initial = profiles[model].trust_factor\n        collector.record_policy_violation\\(model\\)\n        assert profiles[model].trust_factor < initial\n\n    def test_record_policy_violation_unknown_model_no_crash\\(self\\):\n        profiles = {Model.GPT_4O: build_default_profiles\\(\\)[Model.GPT_4O]}\n        collector = TelemetryCollector\\(profiles\\)\n        # KIMI_K2_5 not in profiles â†’ should silently return\n        collector.record_policy_violation\\(Model.KIMI_K2_5\\)\nPYEOF)",
      "Bash(python3:*)",
      "Bash(printf:*)",
      "Bash(ls \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\orchestrator\"\" && ls \"E:DocumentsVibe-CodingAi Orchestratortests\"\")",
      "Bash(curl:*)",
      "Bash(Get-Content \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\projects\\\\analysis_food_delivery.yaml\" -TotalCount 8)",
      "Bash(Get-Content \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\projects\\\\analysis_greek_airbnb.yaml\" -TotalCount 8)",
      "Bash(Get-Content \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\projects\\\\analysis_greek_elections.yaml\" -TotalCount 8)",
      "Bash(Get-Content \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\projects\\\\analysis_greek_real_estate.yaml\" -TotalCount 8)",
      "Bash(Get-Content \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\projects\\\\analysis_netflix_spotify_youtube.yaml\" -TotalCount 8)",
      "Bash(Get-Content \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\projects\\\\analysis_sports_superleague_nba.yaml\" -TotalCount 8)",
      "Bash(Get-Content \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\projects\\\\backend_graphql_api.yaml\" -TotalCount 8)",
      "Bash(Get-Content \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\projects\\\\backend_microservices.yaml\" -TotalCount 8)",
      "Bash(Get-Content \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\projects\\\\backend_rest_api.yaml\" -TotalCount 8)",
      "Bash(Get-Content \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\projects\\\\backend_task_queue.yaml\" -TotalCount 8)",
      "Bash(Get-Content \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\projects\\\\backend_websocket_chat.yaml\" -TotalCount 8)",
      "Bash(Get-Content \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\projects\\\\frontend_nextjs_ecommerce.yaml\" -TotalCount 8)",
      "Bash(Get-Content \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\projects\\\\frontend_react_dashboard.yaml\" -TotalCount 8)",
      "Bash(Get-Content \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\projects\\\\frontend_react_realtime.yaml\" -TotalCount 8)",
      "Bash(Get-Content \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\projects\\\\frontend_svelte_portfolio.yaml\" -TotalCount 8)",
      "Bash(Get-Content \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\projects\\\\frontend_vue_kanban.yaml\" -TotalCount 8)",
      "Bash(Get-Content \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\projects\\\\polytonic_ocr.yaml\" -TotalCount 8)",
      "Bash(Get-Content \"E:\\\\Documents\\\\Vibe-Coding\\\\Ai Orchestrator\\\\projects\\\\polytonic_ocr_fixed.yaml\" -TotalCount 8)",
      "Bash(powershell:*)",
      "Bash(del:*)",
      "mcp__Desktop_Commander__start_process",
      "Bash(find:*)",
      "Bash(wc:*)",
      "Bash(ls:*)",
      "Bash(cat:*)"
    ]
  }
}
